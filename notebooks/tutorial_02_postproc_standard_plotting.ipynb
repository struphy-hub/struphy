{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing and standard plots\n",
    "\n",
    "In this tutorial we learn the basics for handling data generated by Struphy simulations. As an example we will use data generated by the notebook `run_main.ipynb`. You need to run that notebook before engaging in this tutorial. Moreover, you need to be in the same working directory where you launched `run_main.ipynb`. Here, we will\n",
    "\n",
    "1. Look at the generated data from the simulation in `run_main.ipynb`.\n",
    "2. Run the main post processing file `strupy/post_processing/pproc_struphy/main.py`, which is executed upon calling\n",
    "    ```\n",
    "    $ struphy pproc -d DIR\n",
    "    ```\n",
    "    from the console (we will not invoke the console and run directly from the notebook).\n",
    "3. Inspect the data generated during post processing.\n",
    "4. Extract the time grid.\n",
    "5. Look at binning data of the kinetic distribution function (in v and in x-v space).\n",
    "6. Look at 1D snapshots of one component of the magnetic field.\n",
    "\n",
    "At first, we look at the raw data coming from the simulation (before post processing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_out = os.path.join(os.getcwd(), 'struphy_run_main/')\n",
    "\n",
    "os.listdir(path_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two files and one folder named `data/` have been created by the simulation. The file `parameters.yml` is a copy of the parameter file used in the simulation. Let us check the metadata in `meta.txt`, where we can find some useful information about the simulation, such as date of execution, operating system, number of processes etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_out, 'meta.txt')) as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now inspect the content of the `data/` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join(path_out, 'data/')\n",
    "\n",
    "os.listdir(path_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the simulation was ran with only one MPI process from a notebook, only one `.hdf5` file has been created (on process 0). In general, one such file will be created for each process. Let us inpect the content of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(os.path.join(path_data, 'data_proc0.hdf5'), \"r\") as f:\n",
    "    for key in f.keys():\n",
    "        print(key + '/')\n",
    "        for subkey in f[key].keys():\n",
    "            print('    ' + subkey + '/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see five top level keys under which data is stored:\n",
    "\n",
    "1. `feec` stores the finite element coefficients of the electromagnetic fields (only the magentic field 'b2' in this example) and of each fluid species (only one species `mhd` in this case).\n",
    "2. `kinetic` stores, for each kinetic species (only one species `energetic_ions` in this case), the binning data of the distribution function and a certain number of selected marker trajectories (can be specified in the parameters file).\n",
    "3. `restart` stores data in case the simualtion has been interrupted.\n",
    "4. `scalar` stores the scalar quantities of the simulation, such as energies for instance.\n",
    "5. `time` stores the time grid.\n",
    "\n",
    "As a user, we do not need to access the above data directly. This is handled by Struphy's main post processing routine, which should usually be called from the console:\n",
    "\n",
    "        $ struphy pproc -d DIR\n",
    "        \n",
    "Here, we look at this routine in a bit more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from struphy.post_processing.pproc_struphy import main\n",
    "main?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see one manadatory argument, namely the `path` to the simulation output folder. Let us perform the post processing on our example folder. This will perform the following steps:\n",
    "\n",
    "1. Creation of Psydac FemFields.\n",
    "2. Evaluation of fields on the grid specified by `celldivide`.\n",
    "3. Creation of `.vtk` files for further diagnostics in Paraview.\n",
    "4. Evaluation of marker orbits on the mapped domain (Euclidean space).\n",
    "5. Collection of binning data of the distriution function (and evaluation of background in case of df-methods) from all processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(path_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect again the simulation folder after post processing, we see that an additional folder `post_processing` has been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(path_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at its content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(path_out, 'post_processing')\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by extracting the time grid of the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "t_grid = np.load(os.path.join(data_path, 't_grid.npy'))\n",
    "t_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinetic_path = os.path.join(data_path, 'kinetic_data')\n",
    "fluid_path = os.path.join(data_path, 'fields_data')\n",
    "\n",
    "print(os.listdir(kinetic_path))\n",
    "print(os.listdir(fluid_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `kinetic_data` contains one folder for each kinetic species (here just one species `energetic_ions`), whereas `fields_data` contains binary files for physical and logical grids as well as the following folders:\n",
    "\n",
    "1. `em_fields` for the evaluated electromagnetic fields.\n",
    "2. One fluid species `mhd` for the evaluated mhd variables. Note that the magnetic field is stored under `em_fields`.\n",
    "3. `vtk` for viewing FEEC variables in Paraview.\n",
    "\n",
    "Let us work with the data of the `energetic_ions` first. There are two kinds of kinetic data:  \n",
    "\n",
    "1. the binned `distribution_function` ,\n",
    "2. particle `orbits` of selected markers, which can be chosen in the parameter file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_path = os.path.join(kinetic_path, 'energetic_ions')\n",
    "\n",
    "os.listdir(ep_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = os.path.join(ep_path, 'distribution_function')\n",
    "orbits_path = os.path.join(ep_path, 'orbits')\n",
    "\n",
    "print(os.listdir(f_path))\n",
    "print(os.listdir(orbits_path))\n",
    "print(len(os.listdir(orbits_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(os.path.join(f_path, 'v1')))\n",
    "print(os.listdir(os.path.join(f_path, 'e1_v1')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under `distribution_function` we find two folders, one for each binning of $f$ during the simulation. The binning directions have been defined in the parameter file before the run. You can do several binnings during one simulation. For an *n*-dimensional binning in phase space, there are *n+1* `.npy` files created in the respective folder: one file `f_binned.npy` with the binned distribution function, and *n* files with with the corresponding 1d phase space grids in each direction. The naming convention is as follows:\n",
    "\n",
    "1. Suppose we want 1-dimensional binning in the direction `v1` in velocity space. The folder is then called `v1`, the binned distribution function is saved under `v1/f_binned.npy` and the velocity binning grid is saved under `v1/grid_v1.npy`. The same could be done for instance in position space along the coordinate `e2`, which would lead to files `e2/f_binned.npy` and `e2/grid_e2.npy`, respectively.\n",
    "\n",
    "2. Suppose we want to do 2-dimensional binning in the $(e_1,v_1)$ subspace of the phase space. In this case the folder is called `e1_v1`, the distribution function is saved under `e1_v1/f_binned.npy`, the $e_1$-grid is under `e1_v1/grid_e1.npy` and the $v_1$-grid is under `e1_v1/grid_v1.npy`. \n",
    "\n",
    "3. The kind of binnings are defined in the parameter file under `kinetic/<species_name>/save_data/f/slices`. There you can define a list where each string entry defines one binning. For example, `['v1', 'e1_v1', 'e1_e2', 'v1_v2_v3']` would define four binnings in different subspaces of the phase space.\n",
    "\n",
    "Now let's see how the marker `orbits` are stored in Struphy. For each time step *n* (including *n=0*) there is one `.txt` file of the name `<species_name>_n.txt`. This file holds the positions **in physical space** of the designated markers (four in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (os.path.join(orbits_path, 'energetic_ions_01.txt')) as file:\n",
    "    orbit04_str = file.read()\n",
    "    \n",
    "orbit04 = orbit04_str.split('\\n')\n",
    "orbit04"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now plot the binned distribution function. In this example, two binnings have been performed during the simulation, namely `f_v1.npy` and `f_e1_v1.npy`. We shall perform the 1d plots first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_v1 = np.load(os.path.join(f_path, 'v1/', 'grid_v1.npy'))\n",
    "f_binned = np.load(os.path.join(f_path, 'v1/', 'f_binned.npy'))\n",
    "\n",
    "print(grid_v1.shape)\n",
    "print(f_binned.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the first index in the binning data `f_v1.npy` denotes the time index, whereas the second index is the grid index. Let us plot the data at four different instances in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "steps = [0, 1, 2, -1]\n",
    "for n, step in enumerate(steps):\n",
    "    plt.subplot(2, 2, n + 1)\n",
    "    plt.plot(grid_v1, f_binned[step], label=f'time = {t_grid[step]}')\n",
    "    plt.xlabel('v1')\n",
    "    plt.ylabel('f(v1)')\n",
    "    plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2d plots are working in an analogous fashion. Here, we use pyplot's `pcolor` to display 2d data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_e1 = np.load(os.path.join(f_path, 'e1_v1/', 'grid_e1.npy'))\n",
    "grid_v1 = np.load(os.path.join(f_path, 'e1_v1/', 'grid_v1.npy'))\n",
    "f_binned = np.load(os.path.join(f_path, 'e1_v1/', 'f_binned.npy'))\n",
    "\n",
    "print(grid_e1.shape)\n",
    "print(grid_v1.shape)\n",
    "print(f_binned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "steps = [0, 1, 2, -1]\n",
    "for n, step in enumerate(steps):\n",
    "    plt.subplot(2, 2, n + 1)\n",
    "    plt.pcolor(grid_e1, grid_v1, f_binned[step].T, label=f'time = {t_grid[step]}')\n",
    "    plt.xlabel('e1')\n",
    "    plt.ylabel('v1')\n",
    "    plt.title('f(e1, v1)')\n",
    "    plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least we look at some `fields_data`. Since this data is stored in binary format, we use `pickle` for loading. We see that `grids_phy.bin` leads to a list of length three, each entry corresponding to a meshgrid of the respective direction. \n",
    "\n",
    "By contrast, `b2_phy.bin` leads to a dictionary with *n+1* keys, where *n* is the number of time steps saved during the simulation. The keys are the actual time (not the index!) of the time step and the values are lists holding the three components of the magnetic field in physical space (pushed forward 2-form). We shall plot the *z*-component of the *B*-field as a function of *x*, at eight dfferent instances in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(fluid_path, 'grids_phy.bin'), 'rb') as file:\n",
    "    x_grid, y_grid, z_grid = pickle.load(file)\n",
    "    \n",
    "with open(os.path.join(fluid_path, 'em_fields', 'b2_phy.bin'), 'rb') as file:\n",
    "    b2 = pickle.load(file)\n",
    "    \n",
    "print(type(x_grid))\n",
    "print(type(b2))\n",
    "print(x_grid.shape)\n",
    "print(len(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in b2.items():\n",
    "    print(key)\n",
    "    print(type(val))\n",
    "    for va in val:\n",
    "        print(va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "steps = [0, 1, 2, 3, 4, 5, 6, -1]\n",
    "for n, step in enumerate(steps):\n",
    "    t = t_grid[step]\n",
    "    plt.subplot(4, 2, n + 1)\n",
    "    plt.plot(x_grid[:, 0, 0], b2[t][2][:, 0, 0], label=f'time = {t}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('$B_z$(x)')\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
